# -*- coding: utf-8 -*-
"""finl ss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lQ61Ci1lVIJkqNK859w8qpfy9D_gYIO_
"""

import pandas as pd
df = pd.read_csv('reviews.csv')
df.info()  # Data type and null checks
df.head()
# Drop columns that contain any NaN values
df = df.dropna(axis=1)
df.head()
# Check for missing values in each column
missing_values = df.isnull().sum()
print(missing_values)  # Display only columns with missing values

import re

def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove special characters and punctuation
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    # Convert text to lowercase
    text = text.lower()
    return text

# Apply cleaning to the review text column
df['cleaned_text'] = df['reviews.text'].apply(clean_text)

from nltk.corpus import stopwords
# Download stop words if you haven't already
import nltk
nltk.download('stopwords')
# Define stop words
stop_words = set(stopwords.words('english'))

# Function to remove stop words
def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

# Apply stop word removal
df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)

import nltk

# Download the necessary data package
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt_tab') # Download punkt_tab for sentence tokenization
nltk.download('wordnet')

from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Function to map NLTK POS tags to WordNet POS tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to tokenize, POS tag, and lemmatize text
def tokenize_and_lemmatize_with_pos(text):
    tokens = word_tokenize(text)  # Tokenize text
    pos_tags = pos_tag(tokens)  # Get POS tags
    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]
    return ' '.join(lemmatized_tokens)

# Apply the updated lemmatization function
df['cleaned_text'] = df['cleaned_text'].apply(tokenize_and_lemmatize_with_pos)

import gensim
from gensim import corpora
from gensim.models.ldamodel import LdaModel

# 1. Prepare the data:
# Tokenize the text (split into words)
tokenized_texts = [text.split() for text in df['cleaned_text']]

# 2. Create a dictionary:
# This maps each unique word to an ID
dictionary = corpora.Dictionary(tokenized_texts)

# 3. Create the corpus (document-term matrix):
# This converts documents to a bag-of-words representation
corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

# 4. Train the LDA model:
# Specify the number of topics and other parameters
num_topics = 5  # You can adjust this
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10)

# 5. Print the topics:
# Display the top words for each topic
print("\nLDA Model with", num_topics, "Topics:")
for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):  # Adjust num_words as needed
    print("Topic", idx + 1, ":", topic)

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd
import seaborn as sns

# Define the topics with keywords and weights from LDA output
topics = {
    'Topic 1': {'great': 0.014, 'like': 0.013, 'echo': 0.009, 'work': 0.009, 'amazon': 0.009, 'speaker': 0.009, 'thing': 0.009, 'use': 0.008, 'want': 0.008, 'would': 0.008},
    'Topic 2': {'kindle': 0.029, 'fire': 0.012, 'tablet': 0.011, 'ipad': 0.010, 'year': 0.010, 'headphone': 0.010, 'use': 0.009, 'like': 0.009, 'hd': 0.009, 'review': 0.009},
    'Topic 3': {'tv': 0.015, 'amazon': 0.014, 'content': 0.012, 'roku': 0.010, 'use': 0.009, 'box': 0.009, 'fire': 0.008, 'device': 0.008, 'kindle': 0.008, 'like': 0.007},
    'Topic 4': {'prime': 0.027, 'amazon': 0.019, 'movie': 0.018, 'tv': 0.014, 'fire': 0.013, 'im': 0.010, 'free': 0.009, 'kindle': 0.009, 'feature': 0.008, 'comcast': 0.008},
    'Topic 5': {'im': 0.014, 'sound': 0.013, 'year': 0.012, 'ive': 0.011, 'fire': 0.011, 'dont': 0.011, 'amazon': 0.010, 'apple': 0.010, 'one': 0.009, 'headphone': 0.009},
}

# Generate word clouds for each topic
fig, axs = plt.subplots(1, 5, figsize=(20, 4))
for i, (topic, words) in enumerate(topics.items()):
    wordcloud = WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(words)
    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].set_title(topic)
    axs[i].axis('off')
plt.tight_layout()
plt.show()

# Prepare data for bar plot (top 5 words from each topic)
bar_data = []
for topic, words in topics.items():
    for word, weight in sorted(words.items(), key=lambda x: x[1], reverse=True)[:5]:
        bar_data.append({'Topic': topic, 'Word': word, 'Weight': weight})

# Convert to DataFrame for plotting
df_bar_data = pd.DataFrame(bar_data)

# Plot bar chart to compare top words across topics
plt.figure(figsize=(10, 6))
sns.barplot(x='Weight', y='Word', hue='Topic', data=df_bar_data, dodge=True)
plt.title('Top Words per Topic')
plt.xlabel('Weight')
plt.ylabel('Word')
plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

from textblob import TextBlob

# Function to classify sentiment
def get_sentiment(text):
    analysis = TextBlob(text)
    # Classifying sentiment based on polarity
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0:
        return 'neutral'
    else:
        return 'negative'
# Apply sentiment classification to 'cleaned_text'
df['sentiment_label'] = df['cleaned_text'].apply(get_sentiment)

# The 'sentiment_label' column now contains your target variable
print(df[['cleaned_text', 'sentiment_label']].head())

from sklearn.feature_extraction.text import TfidfVectorizer

# Create the TF-IDF vectorizer
tfidf = TfidfVectorizer(stop_words='english', max_features=5000)

# Fit and transform the cleaned text to feature matrix
X = tfidf.fit_transform(df['cleaned_text'])

# Convert target labels to numeric
y = df['sentiment_label'].map({'positive': 2, 'neutral': 1, 'negative': 0})

from sklearn.model_selection import train_test_split

# Train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
print("Logistic Regression:")
print(classification_report(y_test, y_pred_log_reg))
print(f"Accuracy: {accuracy_score(y_test, y_pred_log_reg):.2f}\n")

# Support Vector Classifier (SVC)
svc = SVC(kernel='linear', random_state=42)
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)
print("Support Vector Classifier:")
print(classification_report(y_test, y_pred_svc))
print(f"Accuracy: {accuracy_score(y_test, y_pred_svc):.2f}\n")

# Multinomial Naive Bayes
# Note: MultinomialNB expects non-negative features. Use abs(X_train) and abs(X_test) if your data contains negative values.
mnb = MultinomialNB()
mnb.fit(abs(X_train), y_train)  # Ensure non-negative data for MultinomialNB
y_pred_mnb = mnb.predict(abs(X_test))
print("Multinomial Naive Bayes:")
print(classification_report(y_test, y_pred_mnb))
print(f"Accuracy: {accuracy_score(y_test, y_pred_mnb):.2f}\n")

# Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest Classifier:")
print(classification_report(y_test, y_pred_rf))
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")

from imblearn.over_sampling import SMOTE
from collections import Counter

# Check the class distribution before SMOTE
print("Before SMOTE:", Counter(y_train))

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# Check the class distribution after SMOTE
print("After SMOTE:", Counter(y_train_sm))

# Retrain Logistic Regression on the balanced dataset
log_reg.fit(X_train_sm, y_train_sm)
y_pred_sm = log_reg.predict(X_test)

# Evaluate performance
print("Logistic Regression After SMOTE:")
print(classification_report(y_test, y_pred_sm))
print(f"Accuracy: {accuracy_score(y_test, y_pred_sm):.2f}")

from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid, scoring='f1_macro')
grid.fit(X_train_sm, y_train_sm)

print("Best Parameters:", grid.best_params_)
best_model = grid.best_estimator_

# Train the best Logistic Regression model with C=100
best_model = LogisticRegression(C=100, max_iter=1000, random_state=42)
best_model.fit(X_train_sm, y_train_sm)

# Make predictions on the test set
y_pred_best = best_model.predict(X_test)

# Evaluate the model
from sklearn.metrics import classification_report, accuracy_score

print("Logistic Regression (Best Parameters) After SMOTE:")
print(classification_report(y_test, y_pred_best))
print(f"Accuracy: {accuracy_score(y_test, y_pred_best):.2f}")

# Simplify categories: Handle multiple categories and clean labels
df['primary_category'] = df['categories'].str.split(',').str[0]  # Take the first category if multiple
df = df[df['primary_category'] != "Categories"]  # Remove invalid entries
df['categories'] = df['categories'].str.split(',')  # Split categories into lists
df = df.explode('categories')  # Explode lists into rows
df['categories'] = df['categories'].str.strip()  # Strip whitespace

# Map sentiment labels to numerical values (positive=1, neutral=0, negative=-1)
df['sentiment_score'] = df['sentiment_label'].map({'positive': 1, 'neutral': 0, 'negative': -1})

# Calculate average sentiment score by category
avg_sentiment = df.groupby('categories')['sentiment_score'].mean().sort_values()

# Visualize the average sentiment score by category
avg_sentiment.plot(kind='barh', figsize=(10, 8), color='skyblue')
plt.xlabel('Average Sentiment Score')
plt.ylabel('Product Category')
plt.title('Average Sentiment Score by Product Category')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Function to generate and display a word cloud
def generate_word_cloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

# Filter reviews based on sentiment labels
positive_reviews = df[df['sentiment_label'] == 'positive']['cleaned_text']
negative_reviews = df[df['sentiment_label'] == 'negative']['cleaned_text']
neutral_reviews = df[df['sentiment_label'] == 'neutral']['cleaned_text']

# Combine reviews into a single string for each sentiment
positive_text = " ".join(str(review) for review in positive_reviews)
negative_text = " ".join(str(review) for review in negative_reviews)
neutral_text = " ".join(str(review) for review in neutral_reviews)

# Generate word clouds for each sentiment
generate_word_cloud(positive_text, "Word Cloud for Positive Reviews")
generate_word_cloud(negative_text, "Word Cloud for Negative Reviews")
generate_word_cloud(neutral_text, "Word Cloud for Neutral Reviews")

!pip install streamlit

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models.ldamodel import LdaModel
from gensim import corpora
import nltk

nltk.download('stopwords')
nltk.download('punkt')

# Function to clean and preprocess text
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return ' '.join(tokens)

# Load a sample model (replace with your trained model)
@st.cache_resource
def load_model():
    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
    model = LogisticRegression(max_iter=1000)
    return vectorizer, model

# Main App
def main():
    st.title("Sentiment Analysis App")
    st.sidebar.title("Options")

    # File Upload
    uploaded_file = st.sidebar.file_uploader("Upload your dataset (CSV)", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.write("Uploaded Data")
        st.dataframe(df.head())

        # Preprocess text data
        if 'reviews.text' in df.columns:
            st.subheader("Data Preprocessing")
            df['cleaned_text'] = df['reviews.text'].apply(preprocess_text)
            st.write("Cleaned Data")
            st.dataframe(df[['reviews.text', 'cleaned_text']].head())

            # Sentiment Analysis
            st.subheader("Sentiment Analysis")
            vectorizer, model = load_model()
            X = vectorizer.fit_transform(df['cleaned_text'])
            df['sentiment'] = model.predict(X)
            sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
            df['sentiment_label'] = df['sentiment'].map(sentiment_labels)
            st.write("Sentiment Analysis Results")
            st.dataframe(df[['reviews.text', 'sentiment_label']].head())

            # Visualization
            st.subheader("Visualization")
            sentiment_counts = df['sentiment_label'].value_counts()
            fig, ax = plt.subplots()
            sentiment_counts.plot(kind='bar', color=['red', 'blue', 'green'], ax=ax)
            plt.title("Sentiment Distribution")
            st.pyplot(fig)

            # Topic Modeling
            st.subheader("Topic Modeling")
            tokenized_text = [text.split() for text in df['cleaned_text']]
            dictionary = corpora.Dictionary(tokenized_text)
            corpus = [dictionary.doc2bow(text) for text in tokenized_text]
            lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42)
            topics = lda_model.print_topics()
            st.write("Identified Topics:")
            for idx, topic in enumerate(topics):
                st.write(f"Topic {idx + 1}: {topic}")

            # Word Cloud for Topics
            st.subheader("Word Cloud for Topics")
            for idx, topic in enumerate(topics):
                wordcloud = WordCloud(width=400, height=400, background_color='white').generate_from_text(str(topic))
                st.image(wordcloud.to_array(), caption=f"Topic {idx + 1}")

# Run the app
if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# st.title('Sentiment Analysis App')
# st.write("Hello, World!")
#

from google.colab import files
files.download('app.py')